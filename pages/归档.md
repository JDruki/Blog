## 🔖 文章
	- [Linux vi/vim | 菜鸟教程](https://omnivore.app/me/https-www-runoob-com-linux-linux-vim-html-18ea34eba9d)
	  site:: [runoob.com](https://www.runoob.com/linux/linux-vim.html)
	  date-saved:: [[Apr 3rd, 2024]]
	  id:: 02a51e9b-b062-42e6-b622-d123065c2517
	  collapsed:: true
		- ### 内容
		  collapsed:: true
			- ^^所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。^^
			    
			  ^^但是目前我们使用比较多的是 vim 编辑器。^^  
			    
			  vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。  
			    
			  相关文章：[史上最全Vim快捷键键位图 — 入门到进阶](https://www.runoob.com/w3cnote/all-vim-cheatsheat.html)  
			    
			  ---
- ## 什么是 vim？  
  	    
  	  Vim 是从 vi 发展出来的一个文本编辑器。代码补全、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。   
  	    
  	  简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。   
  	    
  	  连 vim 的官方网站 (<https://www.vim.org/>) 自己也说 vim 是一个程序开发工具而不是文字处理软件。
- ### vim 键盘图  
  	    
  	  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sy152YLEuPnHLk7CBDAFBEhjRvE0cKrNuiumlaSYHtlM/https://www.runoob.com/wp-content/uploads/2015/10/vi-vim-cheat-sheet-sch.gif)  
  	    
  ---
  	    
  	  基本上 vi/vim 共分为三种模式，**命令模式（Command Mode）、输入模式（Insert Mode）和命令行模式（Command-Line Mode）**。
- ### 命令模式  
  	    
  	  **用户刚刚启动 vi/vim，便进入了命令模式。**   
  	    
  	  此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符，比如我们此时按下 i，并不会输入一个字符，i 被当作了一个命令。  
  	    
  	  以下是普通模式常用的几个命令：  
  	    
  		* i -- 切换到输入模式，在光标当前位置开始输入文本。
  		* x -- 删除当前光标所在处的字符。
  		* : -- 切换到底线命令模式，以在最底一行输入命令。
  		* a -- 进入插入模式，在光标下一个位置开始输入文本。
  		* o：在当前行的下方插入一个新行，并进入插入模式。
  		* O -- 在当前行的上方插入一个新行，并进入插入模式。
  		* dd -- 剪切当前行。
  		* yy -- 复制当前行。
  		* p（小写） -- 粘贴剪贴板内容到光标下方。
  		* P（大写）-- 粘贴剪贴板内容到光标上方。
  		* u -- 撤销上一次操作。
  		* Ctrl + r -- 重做上一次撤销的操作。
  		* :w -- 保存文件。
  		* :q -- 退出 Vim 编辑器。
  		* :q! -- 强制退出Vim 编辑器，不保存修改。  
  
  若想要编辑文本，只需要启动 Vim，进入了命令模式，按下 i 切换到输入模式即可。  
  
  命令模式只有一些最基本的命令，因此仍要依靠**底线命令行模式**输入更多命令。
- ### 输入模式  
  
  在命令模式下按下 i 就进入了输入模式，使用 Esc 键可以返回到普通模式。  
  
  在输入模式中，可以使用以下按键：  
  		* **字符按键以及Shift组合**，输入字符
  		* **ENTER**，回车键，换行
  		* **BACK SPACE**，退格键，删除光标前一个字符
  		* **DEL**，删除键，删除光标后一个字符
  		* **方向键**，在文本中移动光标
  		* **HOME**/**END**，移动光标到行首/行尾
  		* **Page Up**/**Page Down**，上/下翻页
  		* **Insert**，切换光标为输入/替换模式，光标将变成竖线/下划线
  		* **ESC**，退出输入模式，切换到命令模式
- ### 底线命令模式  
  
  在命令模式下按下 :（英文冒号）就进入了底线命令模式。  
  
  底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。  
  
  在底线命令模式中，基本的命令有（已经省略了冒号）：  
  		* `:w`：保存文件。
  		* `:q`：退出 Vim 编辑器。
  		* `:wq`：保存文件并退出 Vim 编辑器。
  		* `:q!`：强制退出Vim编辑器，不保存修改。  
  
  按 ESC 键可随时退出底线命令模式。  
  
  简单的说，我们可以将这三个模式想成底下的图标来表示：  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,s7uzS9B-l5JyRsuZ1S8mo-IrKuuR9mXlwPBThxvYG6As/https://www.runoob.com/wp-content/uploads/2014/07/vim-vi-workmodel.png)
  ---
- ## vi/vim 使用实例
- ### 使用 vi/vim 进入一般模式  
  
  如果你想要使用 vi 来建立一个名为 runoob.txt 的文件时，你可以这样做：  
  
  $ vim runoob.txt  
  
  直接输入 **vi 文件名** 就能够进入 vi 的一般模式了。请注意，记得 vi 后面一定要加文件名，不管该文件存在与否！  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sXiYgfTKfHuMBylH0PPj1ohdPCZ4-lC1u5TaKnTtjrnE/https://www.runoob.com/wp-content/uploads/2014/07/078207F0-B204-4464-AAEF-982F45EDDAE9.jpg)
- ### 按下 i 进入输入模式(也称为编辑模式)，开始编辑文字  
  
  在一般模式之中，只要按下 i, o, a 等字符就可以进入输入模式了！  
  
  在编辑模式当中，你可以发现在左下角状态栏中会出现 –INSERT- 的字样，那就是可以输入任意字符的提示。  
  
  这个时候，键盘上除了 **Esc** 这个按键之外，其他的按键都可以视作为一般的输入按钮了，所以你可以进行任何的编辑。  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,stbrbszUi6xabk2V05o8NLmn34cRNH0nA7cxfgZ9aQq8/https://www.runoob.com/wp-content/uploads/2014/07/1C928383-471E-4AF1-A61E-9E2CCBD5A913.jpg)
- ### 按下 ESC 按钮回到一般模式  
  
  好了，假设我已经按照上面的样式给他编辑完毕了，那么应该要如何退出呢？是的！没错！就是给他按下 **Esc** 这个按钮即可！马上你就会发现画面左下角的 – INSERT – 不见了！
- ### 在一般模式中按下 **:wq** 储存后离开 vi  
  
  OK，我们要存档了，存盘并离开的指令很简单，输入 **:wq** 即可保存离开！  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sw1hO63ujWKzPClXVsiJuxZkMHXmsNwlISZuoGrYdgNw/https://www.runoob.com/wp-content/uploads/2014/07/B2FB5146-327C-4019-AC96-DD7A8EE7460C.jpg)  
  
  OK! 这样我们就成功创建了一个 runoob.txt 的文件。
  ---
- ## vi/vim 按键说明  
  
  除了上面简易范例的 i, Esc, :wq 之外，其实 vim 还有非常多的按键可以使用。
- ### 第一部分：一般模式可用的光标移动、复制粘贴、搜索替换等
  		  | 移动光标的方法 | |
  		  |---|---|
  		  | h 或 向左箭头键(←) | 光标向左移动一个字符 |
  		  | j 或 向下箭头键(↓) | 光标向下移动一个字符 |
  		  | k 或 向上箭头键(↑) | 光标向上移动一个字符 |
  		  | l 或 向右箭头键(→) | 光标向右移动一个字符 |
  		  | 如果你将右手放在键盘上的话，你会发现 hjkl 是排列在一起的，因此可以使用这四个按钮来移动光标。 如果想要进行多次移动的话，例如向下移动 30 行，可以使用 "30j" 或 "30↓" 的组合按键， 亦即加上想要进行的次数(数字)后，按下动作即可！ | |
  		  | $$Ctrl$$ + $$f$$ | 屏幕『向下』移动一页，相当于 $$Page Down$$ 按键 (常用) |
  		  | $$Ctrl$$ + $$b$$ | 屏幕『向上』移动一页，相当于 $$Page Up$$ 按键 (常用) |
  		  | $$Ctrl$$ + $$d$$ | 屏幕『向下』移动半页 |
  		  | $$Ctrl$$ + $$u$$ | 屏幕『向上』移动半页 |
  		  | + | 光标移动到非空格符的下一行 |
  		  | - | 光标移动到非空格符的上一行 |
  		  | n<space> | 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20<space> 则光标会向后面移动 20 个字符距离。 |
  		  | 0 或功能键 $$Home$$ | 这是数字『 0 』：移动到这一行的最前面字符处 (常用) |
  		  | $ 或功能键 $$End$$ | 移动到这一行的最后面字符处(常用) |
  		  | H | 光标移动到这个屏幕的最上方那一行的第一个字符 |
  		  | M | 光标移动到这个屏幕的中央那一行的第一个字符 |
  		  | L | 光标移动到这个屏幕的最下方那一行的第一个字符 |
  		  | G | 移动到这个档案的最后一行(常用) |
  		  | nG | n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) |
  		  | gg | 移动到这个档案的第一行，相当于 1G 啊！ (常用) |
  		  | n<Enter> | n 为数字。光标向下移动 n 行(常用) |
  		  | 搜索替换 | |
  		  | /word | 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) |
  		  | ?word | 向光标之上寻找一个字符串名称为 word 的字符串。 |
  		  | n | 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ |
  		  | N | 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 |
  		  | 使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！ | |
  		  | :n1,n2s/word1/word2/g | n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则： 『:100,200s/vbird/VBIRD/g』。(常用) |
  		  | :1,$s/word1/word2/g 或 :%s/word1/word2/g | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) |
  		  | :1,$s/word1/word2/gc 或 :%s/word1/word2/gc | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) |
  		  | 删除、复制与贴上 | |
  		  | x, X | 在一行字当中，x 为向后删除一个字符 (相当于 $$del$$ 按键)， X 为向前删除一个字符(相当于 $$backspace$$ 亦即是退格键) (常用) |
  		  | nx | n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 |
  		  | dd | 剪切游标所在的那一整行(常用)，用 p/P 可以粘贴。 |
  		  | ndd | n 为数字。剪切光标所在的向下 n 行，例如 20dd 则是剪切 20 行(常用)，用 p/P 可以粘贴。 |
  		  | d1G | 删除光标所在到第一行的所有数据 |
  		  | dG | 删除光标所在到最后一行的所有数据 |
  		  | d$ | 删除游标所在处，到该行的最后一个字符 |
  		  | d0 | 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 |
  		  | yy | 复制游标所在的那一行(常用) |
  		  | nyy | n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) |
  		  | y1G | 复制游标所在行到第一行的所有数据 |
  		  | yG | 复制游标所在行到最后一行的所有数据 |
  		  | y0 | 复制光标所在的那个字符到该行行首的所有数据 |
  		  | y$ | 复制光标所在的那个字符到该行行尾的所有数据 |
  		  | p, P | p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) |
  		  | J | 将光标所在行与下一行的数据结合成同一行 |
  		  | c | 重复删除多个数据，例如向下删除 10 行， $$ 10cj $$ |
  		  | u | 复原前一个动作。(常用) |
  		  | $$Ctrl$$ +r | 重做上一个动作。(常用) |
  		  | 这个 u 与 $$Ctrl$$ +r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！ | |
  		  | . | 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) |
- ### 第二部分：一般模式切换到编辑模式的可用的按钮说明
  		  | 进入输入或取代的编辑模式 | |
  		  |---|---|
  		  | i, I | 进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) |
  		  | a, A | 进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) |
  		  | o, O | 进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为在目前光标所在的下一行处输入新的一行； O 为在目前光标所在的上一行处输入新的一行！(常用) |
  		  | r, R | 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) |
  		  | 上面这些按键中，在 vi 画面的左下角处会出现『--INSERT--』或『--REPLACE--』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！ | |
  		  | $$Esc$$ | 退出编辑模式，回到一般模式中(常用) |
- ### 第三部分：一般模式切换到指令行模式的可用的按钮说明
  		  | 指令行的储存、离开等指令 | |
  		  |---|---|
  		  | :w | 将编辑的数据写入硬盘档案中(常用) |
  		  | :w! | 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ |
  		  | :q | 离开 vi (常用) |
  		  | :q! | 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 |
  		  | 注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～ | |
  		  | :wq | 储存后离开，若为 :wq! 则为强制储存后离开 (常用) |
  		  | ZZ | 这是大写的 Z 喔！如果修改过，保存当前文件，然后退出！效果等同于(保存并退出) |
  		  | ZQ | 不保存，强制退出。效果等同于 :q!。 |
  		  | :w $$filename$$ | 将编辑的数据储存成另一个档案（类似另存新档） |
  		  | :r $$filename$$ | 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 |
  		  | :n1,n2 w $$filename$$ | 将 n1 到 n2 的内容储存成 filename 这个档案。 |
  		  | :! command | 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如 『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ |
  		  | vim 环境的变更 | |
  		  | :set nu | 显示行号，设定之后，会在每一行的前缀显示该行的行号 |
  		  | :set nonu | 与 set nu 相反，为取消行号！ |
  
  特别注意，在 vi/vim 中，数字是很有意义的！数字通常代表重复做几次的意思！ 也有可能是代表去到第几个什么什么的意思。  
  
  举例来说，要删除 50 行，则是用 『50dd』 对吧！ 数字加在动作之前，如我要向下移动 20 行呢？那就是『20j』或者是『20↓』即可。
	- ### 高亮
	  collapsed:: true
		- > 所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。  
		  
		  但是目前我们使用比较多的是 vim 编辑器。 [⤴️](https://omnivore.app/me/https-www-runoob-com-linux-linux-vim-html-18ea34eba9d#fcf68815-753d-4f0a-b44d-24362038cc9b)
	- [自建Kubernetes集群完全指南（含云主机公网部署方案）](https://omnivore.app/me/https-www-karsonjo-com-deploy-a-self-hosted-kubernetes-18ea32a7d0a)
	  site:: [KarsonJo妙妙屋](https://www.karsonjo.com/deploy-a-self-hosted-kubernetes/)
	  author:: Karson Zhong
	  date-saved:: [[Apr 3rd, 2024]]
	  date-published:: [[Nov 11th, 2023]]
	  id:: b5e7e5bf-7308-48d1-9f05-541cd7af0e97
	  collapsed:: true
	- ### 高亮
	  collapsed:: true
		- > #### 方法1：MetalLB [⤴️](https://omnivore.app/me/https-www-karsonjo-com-deploy-a-self-hosted-kubernetes-18ea32a7d0a#aad7e249-9f93-4ddb-b239-da29e2bbd64d)
	- ### 内容
	  collapsed:: true
		- ## 说在前面
		    
		  本文介绍如何使用Kubeadm在自己的服务器上将Kubernetes应用部署为**生产环境**，而不使用Kuberentes云服务商。  
		    
		  在进行之前，我建议你先保持理智，问自己为什么需要这么做：  
		    
		  1. 你正在学习Kubernetes，你想知道如何安全地将它部署到生产环境
		  2. 你的应用注重硬件性能，你必须使用硬件机器部署Kubernetes
		  3. 云服务商对你而言太过高昂，你接受使用云主机，以降低可用性、运行效率为代价换取性价比
		  4. ……  
		  
		  总之，如果你有充分的理由，请往下进行；  
		  否则，我非常建议你使用Kubernetes云服务商。它虽然贵，但是很省心。
- ## 准备工作
- ### 服务器  
  
  要按照本文方法部署Kubernetes，你需要准备服务器：  
  		1. 至少两台服务器
  			1. 一台用作Master：至少2G内存
  			2. 一台用作Worker：至少2G内存
  				* 这只是可运行配置，一般而言，运行微服务应用，你可能要为Worker提升配置，且多准备几台Worker
  		2. 这些服务器可以是硬件服务器、也可以是vps
  		3. 如果需要执行高强度数据操作（比如数据库查询），请保证这些机器处于同一内网（如果你使用vps的话）
- ### 应用  
  
  你需要准备能够随时工作于生产环境的Kubernetes应用，包括应用镜像和deployment文件。
- ### 知识  
  
  你需要对Kubernetes和Linux操作有基本了解。
- ## 本文内容
- ### 本文介绍以下内容  
  		1. 配置Docker和容器运行时
  		2. 安装Kubeadm
  		3. 创建kubernetes集群
  		4. 配置持久存储
  		5. 配置ingress（公网访问）
  		6. 配置https
- ### 本文不会展开以下内容  
  		1. 将Kubernetes部署到云服务商
  		2. 如何编写Kubernetes应用
  		3. 如何编写Kubernetes deployment
- ### 演示操作环境  
  		1. 使用Debian12作为所有机器的操作系统
  		2. 所有机器是vps，Master和Worker不在同一内网，但Worker彼此在同一内网
  			* Master 1台 是阿里云ecs
  			* Worker 3台 是国外vps
  			  > 我这组合可算是把什么💩都踩了一遍
- ## 准备工作
- ## 更换纯净的系统镜像（非必须）  
  
  哥们不是开玩笑的。  
  
  如果你的系统镜像被服务商进行了魔改，建议先安装纯净的系统  
  比如阿里云迷之Debian12  
  
  Debian官方已经在12中把`sysetmd.resolved`给移除了  
  但阿里云Debian12直接把`sysetmd.resolved`作为默认DNS服务😅  
  
  也不是不行，但你要保证所有机器都使用这个。不然会出问题  
  如果你其它机器都是纯净系统的，我建议把阿里云那台给重装掉算了
- ## 重启所有机器  
  
  某些镜像在第一次重启之前可能存在问题。如果你的机器从来没重启过，我建议先重启一遍。
- ## 网络连通性  
  
  **非常重要！**  
  在开始前，先检测各个机器之间的连接性。  
  
  然后检查所有机器的端口开启情况。某些防火墙或者云服务器会默认关闭端口，你需要确保对应机器的以下端口是可以**公开**访问的：
- ### Master结点
  			  | Protocol | Direction | Port Range | Purpose | Used By |
  			  |---|---|---|---|---|
  			  | TCP | Inbound | 6443 | Kubernetes API server | All |
  			  | TCP | Inbound | 2379-2380 | etcd server client API | kube-apiserver, etcd |
  			  | TCP | Inbound | 10250 | Kubelet API | Self, Control plane |
  			  | TCP | Inbound | 10259 | kube-scheduler | Self |
  			  | TCP | Inbound | 10257 | kube-controller-manager | Self |
- ### Worker结点
  			  | Protocol | Direction | Port Range | Purpose | Used By |
  			  |---|---|---|---|---|
  			  | TCP | Inbound | 10250 | Kubelet API | Self, Control plane |
  			  | TCP | Inbound | 30000-32767 | NodePort Services (default) | All |
  			  > 详情请见Kubernetes文档 [Ports and Protocols](https://kubernetes.io/docs/reference/networking/ports-and-protocols/)  
  **提示：**  
  		1. 为了避免意外，我的建议是先开放全部端口，搞定了再关闭不需要的端口。
  		2. 请仔细检查防火墙设置，并且尝试在服务器之间测试tcp连接
  		  > 你可以跳过这步，但如果出了问题，请不要忘记排查防火墙/安全组。  
  提供一个检测tcp连接的方法：  
  A主机作为服务端，开启端口的服务  
  
  B主机作为客户端，接通端口  
  
  发点东西过去看看A能不能收到
- ## 关闭swap  
  
  Kubernetes暂时无法直接运行于开启交换区（虚拟内存）的环境，因此请确保你关闭了所有交换区。  
  
  查看是否有交换区（swap容量不为0）：  
  
  关闭交换区直到关机/重启：  
  
  禁止开机启动交换区：  
  打开以下文件  
  
  将里面所有交换区注释掉，示意：
- ## 同步所有服务器上的时间  
  
  如果服务器的时间不准确，可能会造成证书过期，CoreDNS失联等问题。  
  所以请务必保证所有服务器处于相同时区，并且与时间服务器同步时间。  
  
  设置系统时区  
  
  设置本地时钟使用UTC时间  
  
  重启相关服务
- ## 更改服务器的主机名称（非必须）  
  
  服务器的主机名称(hostname)将成为Kubernetes的结点名称。  
  名称无法（很难）在部署后更改，请在部署前决定每个机器的名称。  
  
  建议：  
  		1. 主机名应识别出是Master还是Worker
  		2. 如果你的服务器数量很少，你可以使用数字作为结点后缀
  		3. 如果服务器很多，而且每个配置不同，建议附加简洁的配置信息，或者用途
  		4. 总之你需要一眼就能认出这是哪个服务器
  		5. 主机名绝对**不能**重复
- ### 如何更改
- #### vps  
  
  如果你是云服务商的vps机器，先看看控制面板能不能改
- #### 硬件服务器  
  
  如果不能改或者硬件主机，按以下操作：  
  
  修改主机名  
  
  修改hosts文件  
  
  将里面出现的所有旧名称项，都替换为新主机名
  		  > **注意：** 某些主机的hosts文件会有托管，此时直接修改hosts无效。请注意hosts文件是否有相关注释说明。  
  重新登陆ssh就可以看到新主机名
- ## 安装docker和containerd容器运行时
  		  > 这一步需要在Master和Worker进行  
  请参考[官方教程](https://docs.docker.com/engine/install/debian/)进行安装  
  
  按照教程安装完后，你应该会同时安装完docker和containerd
- ## 验证安装  
  
  验证docker和containerd成功安装并启动：
- ## 配置containerd
  		  > 此步具有时效性，只保证编写时刻（2023-11-5）需要如此操作。  
  		1. containerd安装时默认关闭cri（容器运行时）插件，需要手动打开
  		2. 另外kuberadm需要使用cgroup v2，因此需要开启containerd的cgroup v2支持  
  
  打开`/etc/containerd/config.toml`文件，作两处修改  
  		1. 将`cri`从`disabled_plugins`中移除
  		1. 在文件中加入以下内容以启用cgroup v2  
  
  编辑完毕后重启服务
- ## 安装kubeadm
  		  > 这一步需要在Master和Worker进行  
  请参考[官方教程](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\#installing-kubeadm-kubelet-and-kubectl)进行安装
- ## 验证安装  
  
  验证已经安装kubeadm、kubelet和kubectl
- ## 配置vpc网络的外部访问
  		  > 这步仅需要在满足以下条件的机器上执行：  
  		  1. 机器希望以公网身份接入网络  
  		  2. 机器网卡的IP并不是机器的公网IP  
  ## 检测公网IP是否挂载在网卡上  
  
  请输入以下命令检测默认网卡是否为公网IP  
  
  或者使用以下命令，观察默认网卡（比如eth0）是否存在你的公网IP  
  
  如果显示的是你的公网IP，或者你不需要公网接入，你可以跳过这一节；否则，继续往下执行。
- ## 创建公网IP的虚拟网卡  
  
  输入以下命令，为默认网卡增加临时的IP地址。这里假设你的默认网卡名为`eth0`，如果不是，**你需要修改**：
  		  > 这将持续到机器重启，重启后你需要重新创建。  
  关于持久化……每个镜像使用的网络服务不同，会有不同的搞法，这里就不展开了  
  就单纯给点提示吧：  
  		1. 确定自己系统使用的是什么网络服务
  		1. 谷歌一下这个东西怎么增加IP地址
- ## 验证IP是否生效  
  
  使用以下命令，检测公网IP是否出现在你的默认网卡中
- ## 修改kubeadm启动kubelet时使用的ip  
  
  查看kubelet的环境变量文件：  
  
  你会看到类似以下的输出：  
  
  kubeadm官方说第一个文件是自动生成的，不建议修改。因此我们修改第二个（不存在则创建）  
  
  在打开的文件中，指定`--node-ip`参数为公网IP
  		  > 关于这点的更多[阅读资料](https://github.com/kubernetes/kubeadm/issues/203)  
  		  > 这一步只需要在Master进行  
  **重要提醒：**  
  中途出现问题可能会导致kubeadm初始化中断，你可以使用`kubeadm reset`重置到初始前状态
- ## 配置仓库镜像
  		  > 这步需要在无法连接到`k8s.gcr.io`的Master执行（大陆机器）  
  如果是国内环境，很遗憾，无法访问默认的`k8s.gcr.io`容器仓库。  
  没有关系😭，我们可以使用镜像仓库  
  此处需要将容器仓库修改为国内镜像
  		  > 如果你想了解更多，请[参考这里](https://segmentfault.com/a/1190000038248999)：  
  ### 配置containerd sandbox-image的镜像  
  
  我们使用阿里云作为镜像服务器  
  
  打开containerd配置文件  
  
  在文件中加入以下结点：
  		  > 注意，如果文件中有相同的结点，将`sandbox_image`附加至下面即可，不要创建重复结点。  
  ## 选择网络插件  
  
  网络插件是为集群提供网络实现的部件。  
  因为某些网络插件需要在初始化时指定cidr网段，因此初始化前你需要决定使用哪个插件。  
  常用的网络插件有：calico、flannel、weave
- ### 使用哪个网络插件  
  
  **并不是所有网络插件都适用于你的网络环境。**  
  		1. 一般而言calico是最老牌的
  		2. 但是在Master为阿里云的环境，以**公网组网**为基础，我亲测：
  			1. 目前calico无法正常安装使用，已在多个地方得证
  			2. flannel可以正常安装，但经我测试（阿里云Master+美国机Worker）无法使用。会出现各种迷惑问题
  				* 内部DNS服务器可能完全失效
  				* 或者永远只有部分node能够使用内部DNS解析
  				* 访问外部资源TLS握手失败，错误代码40（`SSL alert number 40`）  
  
  总结：  
  		1. 如果你是类似阿里云这种公网IP不在网卡的环境，且你希望公网组网，请使用**weave**
  		2. 除此以外，请使用**calico**
  		3. 当你尝试其它网络插件无效时，请尝试flannel  
  
  我想说的是：如果你（我😅）折腾一个网络插件很久，重装多很多次都不行，也有可能是该插件本身不适用于你的网络环境。不妨换个试试。
- ## 初始化  
  
  使用适当的参数初始化kubernetes：  
  		1. `--pod-network-cidr`是设置后面使用的网络插件的网段，请按照以下说明填写默认网段。
  			* [calico](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart): `192.168.0.0/16`
  			* [flannel](https://github.com/flannel-io/flannel): `10.244.0.0/16`
  			* [weave](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/): 请**删除**该参数
  			* [其它网络插件](https://kubernetes.io/docs/concepts/cluster-administration/addons/\#networking-and-network-policy)：请阅读相应文档
  		2. `--image-repository`: 使用阿里云镜像
  			* 如果机器无法访问`k8s.gcr.io`需要填写。
  		3. `--apiserver-advertise-address`: 指定API Server的监听地址。
  			* 如果你的机器处于vpc网络（公网IP不在网卡上），且你希望以公网形式加入集群，需要在创建了虚拟网卡的基础上，填写公网IP。
  		4. 其它可配置参数请看[官方文档](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/)  
  
  指令示意：
- ## 启动集群  
  
  在上一步顺利完成后，会显示以下信息：  
  
  然而无论你是否为root用户，都请按照这个提示启动kubernetes集群：  
  
  而且请记住最后输出的加入token，workers需要使用这个加入集群。（如果没记住也不慌，后文教你重新生成）
- ## 验证集群状态  
  
  验证状态：  
  
  观察结点状态：  
  
  此处`NoReady`是正常现象，因为我们还没安装网络插件
- ## 存档点！  
  
  恭喜你已经成功渡过第一阶段，你还好吗😊？  
  接下来需要进行kubernetes内部的配置，在此之前我需要做一些提醒：
- ## 如何排错  
  
  在接下来的过程中，由于环境问题你可能会出现各种各样的状态异常、容器Crash。本文不可能涵盖所有的情况，但我会给你解决问题的方向。  
  出现这种问题是你应该：  
  		1. 首先使用`kubectl get pods -A -o wide`等命令查看出错的容器。
  			1. 使用`kubectl describe pod <pod-name> -n <namespace>`查看错误的信息
  			2. 使用`kubectl logs <pod-name> -n <namespace>查看容器日志`
  		2. 得到错误信息后，请善用搜索引擎和社区解决问题
  		3. 如果你使用的国内的服务器，建议你也查看国内社区的信息。因为这可能涉及到主机服务商虚拟化技术不同导致的，就如上面提到的阿里；也有可能说是某种不可抗力导致的，比如……
- ## 做好准备  
  
  因为下一步“安装网络插件”对环境是破坏性的。如果安装失败了，它很难简单使用`kubeadm reset`重置。它会给你的机器引入永久性的改变……  
  
  我想说的是，如果网络插件安装失败、或者想改变使用的网络插件，你可能会陷入一种“无论如何配置都不行”的境地。  
  
  你仍然可以反复尝试，但我需要提醒的是：如果你多次尝试都失败，很可能是机器环境已经被网络插件改变过了，你可能需要回滚这些设置。  
  
  所以：  
  		1. 如果你有条件，可以考虑在此时创建一个系统镜像，以方便回滚。
  		2. 如果不行，你可以查找如何彻底卸载网络插件的方法， 这个东西因插件、版本而异。
  		3. 但较简单的做法，可能是重装系统，然后重新执行一次上述步骤。
- ## 部分地区网络连接问题  
  
  下面的步骤我们需要拉取各种容器镜像。  
  
  由于部分镜像需要在`registry.k8s.io`或`quay.io`下载。然而这几个源在天朝强大的防火墙下已经无法访问了。  
  不同于`hub.docker.com`，这些网站是无法通过docker配置镜像解决的……  
  
  得益于此，要拉取镜像，你需要化简为繁：  
  		1. 寻找一个镜像站
  		2. 从镜像站下载镜像
  		3. 将下载的镜像tag为`registry.k8s.io/xxx...`
  		4. 修改部署的yaml文件，将每个`ImagePullPolicy`修改成`Always`以外的值，使其使用本地镜像  
  
  至此，我是完全投降了……  
  虽然不是不能做，而是实在太麻烦了！！！  
  以后还有其它需要`registry.k8s.io/xxx...`的容器怎么办，又重复这四步吗？？？  
  
  我的建议是，收手吧阿祖，外面全是防火墙😅  
  如果没有**魔法**，我建议你选一个只在`hub.docker.com`拉镜像的方案  
  
  **因此，国内Worker机器的部署请自求多福，并没有一劳永逸的做法，这个东西会非常麻烦**  
  下面的操作如果涉及内陆无法下载的源，我会尽量提醒
  		  > 好了，该说的就这么多了，祝你好运！  
  ## 安装网络插件
  		  > 这一步只需要在Master进行。你只需要选择其中一个安装  
  ## 安装calico  
  
  你需要安装operator和calico本身。  
  
  **注意：**如果安装步骤让你`taint nodes`。这一步是可选的。执行该命令后将允许pod被分配到Master中，一般不建议这么做。建议跳过这句。  
  
  请按照[官方指引](https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart)进行安装
- ## 安装flannel  
  
  本节介绍安装[flannel](https://github.com/flannel-io/flannel)。  
  
  如果你不需要进行额外配置，直接下载部署即可：  
  
  如果需要改变默认配置，你可以先wget下来，修改后再apply。
- ## 安装weave  
  
  安装weave通常也只需要**apply一个文件**。  
  类似于……  
  
  由于版本号可能会改变，请前往[官网获取最新的地址](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/)
- ## 验证node准备完毕  
  
  首先等待所有pod安装完毕  
  
  按`ctrl+c`退出  
  
  然后执行以下命令验证node准备完毕：
- ## Worker加入集群
  		  > 这一步只需要在每个Worker进行  
  ## 生成连接串  
  
  如果你没有记录连接串，或者连接串已过期，可以在**Master**结点使用以下命令创建连接串：
- ## 加入集群  
  
  使用生成的join命令加入集群：
- ## 等待集群准备完毕  
  
  直到所有pod为`Running`，所有node为`Ready`
- ## 验证DNS服务可用  
  
  该步验证你的Worker可以使用到dns解析  
  
  **注意：**本例只保证“某个Worker能够使用CoreDNS”，即CoreDNS基本正常工作；但不代表CoreDNS在所有Worker、Master都正常工作。  
  如果你想验证后者，请在此基础上进行更多的验证。  
  
  查看目前所有的service：  
  
  在某个Worker中启动`busybox:1.28`（不能使用更高版本镜像）：  
  
  在该pod中，尝试`nslookup`所有服务，比如：  
  
  如果输出大致如下，则说明该Worker可以连接DNS服务器：  
  
  如果输出无法解析，请参考[官方文档进行DNS排错](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)  
  确保不存在问题，再进行下一步；如果问题实在不能解决，请考虑更换网络插件
- ## 配置持久存储（Persistent Volume）
  		  > 如果你的应用需要存储状态（使用数据库等）需要执行  
  ## 选择哪种持久存储？  
  
  Kubernetes的持久存储卷 **（Persistent Volume，下简称PV）** 可以大致分为两类：  
  		* 网络存储
  		* 本地存储  
  
  一般使用网络存储，因为它与pod所在的node无关。无论pod被部署到哪个结点，它都能访问到位于网络的持久存储卷。  
  
  但是如果你的应用需要高强度IO，你可能需要使用本地存储。Kubernetes本地存储插件主要有两个，生产环境中请使用`local`而非`hostPath`。  
  `local`虽然比`hostPath`更好，但也有明显的缺点：  
  		1. pod会被绑定至存储所在的node
  		2. 必须手动分配空间，无法使用动态空间分配  
  
  所以请权衡性能与高可用性。  
  对于内网环境中，一般情况下使用网络存储并不会有明显的IO瓶颈。因此本文配置网络存储。
- ## 数据冗余  
  
  存储冗余可以保证数据不会丢失。  
  冗余可以产生在两个层面：  
  		1. 存储层面：可以在将文件写入到PV时就将数据存储多份
  		2. 应用层面：比如镜像数据库和分片数据库  
  
  在存储层进行冗余并不是没有缺点，它会导致占用的存储空间成倍增长。  
  而且存储层的冗余并不能带来数据的高可用性。因为如果你只有一个数据库实例，在这个数据库宕机时，服务仍然无法访问（当然数据是不会丢失）。  
  
  所以如果你打算为高可用而组建分布式数据库，存在多个数据库实例，其实数据库本身就给你产生了冗余。此时在底层再次冗余只是浪费空间。
- ### 是否需要在PV层存储冗余？  
  
  对PV而言，本节主要讨论你是否需要在PV层面进行冗余。  
  
  现在，假设你的数据非常宝贵，你不希望丢失数据。  
  你打算将数据存储到自己的服务器中，而且你至少存在两个Worker，否则白搭。  
  		1. 如果你使用的是裸机组网，并且裸机的存储不存在RAID，即底层不存在任何冗余。那你需要在PV层使用冗余。
  		2. 如果你使用的是云服务主机，并且你**确信**云服务主机已经为你提供了底层冗余，即只要它不跑路，你的数据不可能丢。那你不必在PV层使用冗余。
  		3. 如果你打算在应用层（数据库）对数据进行冗余，此时在底层冗余也是不必要的。
- ## 安装持久存储
- ### 云原生存储  
  
  云原生存储是专为容器环境打造的存储方案。在Kubernetes中，推荐使用云原生存储，而非直接操作文件系统。  
  云原生存储带来的好处是：  
  		1. 它本身也部署于Kubernetes中，比较方便
  		2. 它为你管理、分配存储资源
  		3. 大部分方案提供备份、数据恢复、数据冗余等功能
- #### 云原生存储一览  
  
  目前比较火的有这些：  
  		1. [OpenEBS](https://github.com/openebs/openebs)
  			* 支持多种存储方式，提供多种存储引擎
  			* 配置较简单
  		2. [Longhorn](https://github.com/longhorn/longhorn)
  			* 主要是远程分布式存储
  			* 提供图形界面进行管理
  		3. [Rook](https://github.com/rook/rook)
  			* 没有用过，据说适合中大型集群
  			* 最好为它提供未分区的硬盘空间
  		4. [GlusterFS](https://github.com/gluster/glusterfs)
  			* 没有用过  
  
  选一个用吧，我这里提供OpenEBS和Longhorn的安装方法
- ### 配置Longhorn
- #### 部署Longhorn  
  
  请按照官方文档安装longhorn  
  
  主要进行两步：  
  		1. [安装先决条件](https://longhorn.io/docs/latest/deploy/install/\#installation-requirements)
  			* 主要是iscsi和nfs
  			* 需要在每个需要Longhorn分配空间的机器上执行
  		2. [安装Longhorn](https://longhorn.io/docs/latest/deploy/install/install-with-kubectl/)  
  
  等待所有容器安装完毕：
- #### 自定义存储类  
  
  longhorn会为你添加一个默认的存储类，输入以下命令查看  
  
  这个存储类会默认**存储3份冗余**，如果这是你想要的，可以跳过。  
  如果不是，请自行设置默认存储类
  ---
  
  首先从github中下载storage class的模板  
  
  打开，修改模板：  
  		* 修改`metadata.name`为你喜欢的名称
  		* 将里面的`numberOfReplicas:`修改为你所需的数量，这决定冗余存储的份数
  		* 其它选项请查阅官方文档  
  
  应用这个存储类  
  
  确认该存储类正确创建  
  
  输出类似以下：
- #### UI界面  
  
  需要借助ingress，请在后文配置好ingress访问后，按照[官方文档](https://longhorn.io/docs/latest/deploy/accessing-the-ui/longhorn-ingress/)操作。
- ### 配置OpenEBS
  		  > **注意：**OpenEBS的部分镜像源无法在内陆访问，请自行权衡  
  我选择安装OpenEBS，当然你可以选择其它的
- #### 安装iSCSI
  		  > 这一步需要在每个需要OpenEBS分配空间的机器上执行  
  		  如果你使用默认的配置安装，那OpenEBS会在所有Worker上分配  
  OpenEBS需要iSCSI支持，这个需要安装在系统上，[不同系统的安装方法不一，可参考这里](https://openebs.io/docs/user-guides/prerequisites)  
  
  在debian中，你可以使用以下命令检测iSCSI是否启动：  
  
  如果找到服务，并且启动，你可以跳到下一步  
  如果找到服务，但已关闭，请启动：  
  
  如果没找到服务，请安装：  
  
  最后，检测服务确实正常运行：
- #### 部署OpenEBS  
  
  按[官网指示](https://openebs.io/docs/user-guides/installation)安装`OpenEBS Operator`以及需要的存储支持  
  
  OpenEBS提供两种存储方案：  
  		1. 本地存储
  		2. 镜像重复存储（远程存储）  
  
  首先查看默认的安装给你装了什么？  
  
  如果没出现你想要的engine，你需要自己安装：  
  两种方案均有多种存储引擎可供选择，你可以根据自己的需求安装。  
  比如，你需要使用远程存储，你可以安装`cStor`，`Jiva`，`Mayastor`中的一个。
- ### 使用Jiva引擎建立持久存储  
  
  我这里安装Jiva作为远程存储引擎  
  安装的具体步骤请参考[官网指示](https://openebs.io/docs/user-guides/jiva/jiva-install)  
  主要做三个事情：  
  		1. 安装Jiva引擎
  		2. 配置Jiva
  			* 关于Jiva的镜像重复数（replicationFactor），请按需求填写，一般不超过安装了OpenEBS的Worker数
  		3. 创建使用Jiva的Storage Class  
  
  提醒：创建完Policy和Storage Class记得apply  
  
  最后，检测storage class正确创建：  
  
  输出应该存在刚刚创建的storage class：  
  
  由于Jiva Storage Class是支持动态提供（Dynamic Provisioning）的，因此我们不需要手动创建PV
- ## 设置默认存储类（可选）
  		  > 设置默认存储类可以使其它应用在不指明存储类的时候也能使用持久存储。非常建议为集群设置一个默认存储类。  
  		  但这并不是必须的  
  默认存储类会在`kubectl get sc`的输出中显示`default`字样  
  
  首先我们查看所有存储类  
  
  如果你发现以下情况，建议往下执行：  
  		1. 没有默认存储类
  		2. 默认存储类不是你想要的  
  
  我们可以使用以下指令设置/取消一个默认存储类：  
  
  注意`<storage-class-name>`填写存储类的名字；最后的布尔值控制是设置还是取消。  
  
  设置后再次检测storage class的情况，输出应如下。 请确保**至多只有1个**默认存储：
- ## 配置外网入口
  		  > 如果你需要支持公网访问，你需要进行这一步  
  本节将使用ingress nginx controller配置公网访问。
- ## 安装ingress nginx controller  
  
  安装ingress非常简单，根据[官方教程](https://kubernetes.github.io/ingress-nginx/deploy/\#quick-start)，使用deployment安装：  
  
  输入以下命令等待容器创建完毕  
  
  最后你会看到类似这样的结果，只要看到controller为`Running`就是正常：
- ### 解决无法访问  
  
  此时查看支持ingress正常运作的LoadBalancer服务：  
  
  你会发现输出如下，`EXTERNAL-IP`永远为`pending`：  
  
  随之你会发现ingress nginx controller无法外部访问……  
  
  这是因为自建Kubernetes与云环境不同：自建集群并不自带负载均衡器实现，如果你此时创建`LoadBalancer`，将永远处于`pending`状态。  
  
  对此我们有两种做法：  
  		1. 安装负载均衡实现
  		2. 不使用负载均衡直接访问  
  
  下面将介绍这两种做法，注意，并不是所有做法都适合你的情况，请注意阅读后续提及的先决条件和弊端。  
  
  **注意：**总共有3种做法，选择1种执行即可。
  		  > 这些做法出自ingress nginx[官方文档](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/)  
  ### 安装负载均衡
- #### ^^方法1：MetalLB^^
  		  > **^^裸^^机配置首选！！！**  
  
  		  该方法**只适合**所有结点（至少是所有Worker结点）处于同一内网。  
  		  也就是：硬件裸机，或者某些支持组内网的云服务商。（但我发现某些国外的vps商若处于同一机房也能用）  
  首先，请按照[官方指南安装metallb](https://metallb.universe.tf/installation/)  
  
  然后，配置IP池。最简单且内网通用的做法是使用Layer 2（数据链路）模式  
  你只需要将你的**公网IP段**加入IP池就好。  
  
  简单来说就是把你希望作为外网访问入口的IP段加入到IP池，MetalLB会将这些IP依次分配给出现的LoadBalancer  
  网段支持子网写法和范围写法  
  如果你的服务器只有单个IP，可以视为只有单个IP的子网`<ip-address>/32`  
  
  写法示意如下：
  		  > 由于LoadBalancer是交通枢纽。如果你的服务器各自有流量限制，请尽量选择流量充足的服务器写进IP池。  
  因为MetalLB版本迭代非常快，因此这里也不放具体的配置了，请按照[官方指南](https://metallb.universe.tf/configuration/\#defining-the-ips-to-assign-to-the-load-balancer-services)进行配置。
  ---
  
  **错误排查**  
  你可能会遇到webhook错误，请尝试将MetalLB的Controller转移到Master结点。  
  错误原因不明，详情请参考[这个issue](https://github.com/metallb/metallb/issues/1597\#issuecomment-1636690287)。
  ---
  
  执行结束后，输入以下命令可以查看nginx ingress的LoadBalancer IP：  
  
  如无意外，此时LoadBalancer显示的将是一个公网IP。
- #### 方法2：外部LoadBalancer  
  
  效仿云服务，我们也可以自己整一个外部负载均衡器。这个负载均衡器可以是硬件或软件。  
  软件负载均衡器，推荐使用[HAProxy](http://docs.haproxy.org/)  
  
  但这个方法我没试，看起来非常专业，有兴趣的朋友可以配置一下😘
- ### 直接连接
- #### 方法3：暴露主机网络  
  
  我们也可以不使用任何LoadBalancer，ingress nginx controller提供了直接对外访问的做法。  
  
  首先我们将部署文件下载下来：  
  
  在里面找到`kind: Deployment`的项，加入以下配置项（注意不要引入重复的项）：  
  
  保存退出后，将其应用：  
  
  此时检查nginx controller的情况：  
  
  你会发现nginx controller的IP变成了pod所在node的公网IP（前提是你的node真的有公网IP）  
  
  **注意事项：**  
  		1. 每个node最多只能运行一个nginx controller，因为它使用到了宿主机的资源。你无法将同一端口分配给两个nginx实例。
  		2. 此时nginx controller的IP地址取决于其运行的node，这个IP可能并不是你所期望的。
  ---
  
  那么怎么指定访问nginx的公网IP呢？  
  只能间接指定，有两个做法：  
  
  做法1：使用结点选择/亲和性配置，让nginx controller优先创建于你指定的node  
  首先为希望运行ingress的node创建一个`type`为`ingress`的标签（你可以更改为任意的键值对）  
  
  在部署nginx controller前，修改Deployment  
  在里面找到`kind: Deployment`的项，加入以下配置项（注意不要引入重复的项）：  
  
  修改完后重新apply  
  你可能需要将现有的ingress-nginx-controller pod删除以触发重新调度
  ---
  
  做法2：使用daemon set部署nginx controller，让每个node（包括Master）都运行nginx controller  
  		* 如果你希望健壮一点，可能这个比较好
  		* 当然它的运行开销也更大
- ## 验证外网访问  
  
  使用上步暴露的公网IP（各个做法不同），在外部浏览器中尝试进行访问。  
  如果显示`nginx 404`页面，说明配置正确。
- ## 部署你的APP  
  
  恭喜，到这一步你已经配置好生产环境下使用的Kubernetes了  
  接下来你只需要：  
  		1. 将你的APP部署到Kubernetes
  		2. 编写Ingress文件将流量从ingress nginx转发到应用中
  			* 请参考[ingress nginx官方文档](https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/)
  			* 注意：如果你使用IP访问，请**不要**指明`spec.rules.host`；如果使用域名访问，请**务必**指明
  		3. 访问自己的应用，确认已正常运作
- ## 配置域名和https
  		  > 非必须，单如果是公开访问的应用，建议进行  
  这一步你需要准备：  
  		1. 一个解析至你应用的域名
  		2. 一个合法的TLS证书  
  
  首先将证书内容复制到服务器，比如我的：  
  
  创建一个包含证书内容的secret  
  
  修改你的ingress（nginx）部署文件，加入以下结点：  
  
  重新apply  
  
  使用`https://mysite.com`访问你的站点，确认TLS证书已生效
- ## 通关  
  
  大功告成！至此我们终于成功自建了Kubernetes网络  
  这一路上并不容易，但这仅仅只是一个开始  
  你还需要关注如何进行维护、备份和迁移……  
  
  但到此我有一个疑问  
  Docker、Kubernetes的出现不是为了简化应用部署的过程吗？  
  但搞起来怎么这么复杂呢？  
  
  程序员的工作是降低了，但运维真就哭了……  
  但无所谓，反正我这种oneman从来都是一个人全部搞定，四舍五入没节省任何时间😭
  ---
  
  好吧，牢骚是发到这……  
  其实Kubernetes部署到部分云服务商还是很舒服的，只是我看到那个价格就已经饱了  
  所以自建Kubernetes虽然能达到生产环境的要求，但需要额外的维护！  
  除非你真的有充分的理由，否则云服务商还是一个不错的选择啊（除了很贵😅😅😅）  
  
  [ ![](https://proxy-prod.omnivore-image-cache.app/0x0,sPUqb_xSMMD0DwEBxxxGv2zQsPZKvSRO0NdWOQoTGDWo/https://cravatar.cn/avatar/dd9445633e169ac5d2a8eead1c64cc74?s=96&d=mm&r=g) ](https://www.karsonjo.com/author/mkr67n/)
- ###  
  
  程序员，热衷于游戏开发和软件制作。但也是一个杂食动物，喜欢探索各种赛博相关的奇技淫巧。  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sAI4h15wgpkLVfmHHbbff6TR9CKJBCgA2jb0eB8VYHMY/https://www.karsonjo.com/wp-content/plugins/wpdiscuz/assets/img/loading.gif)
	- [使用 kubespray 搭建集群 | Kubernetes 实践指南](https://omnivore.app/me/kubespray-kubernetes-18e749f6773)
	  site:: [imroc.cc](https://imroc.cc/kubernetes/deploy/kubespray/install)
	  date-saved:: [[Apr 3rd, 2024]]
	  id:: b3f69ab4-918b-440c-9657-a7ce513c9238
	  collapsed:: true
		- ### 内容
			- ## 原理[​](\#原理 "原理的直接链接")
			    
			  [kubespray](https://github.com/kubernetes-sigs/kubespray) 是利用 [ansible](https://docs.ansible.com/ansible/latest/index.html) 这个工具，通过 SSH 协议批量让指定远程机器执行一系列脚本，安装各种组件，完成 K8S 集群搭建。
- ## 准备工作[​](\#准备工作 "准备工作的直接链接")  
  	    
  	  ^^下载 kubespray 并拷贝一份配置:^^  
  	    
  	  `\# 下载 kubespray $ git clone --depth=1 https://github.com/kubernetes-sigs/kubespray.git $ cd kubespray \# 安装依赖，包括 ansible $ sudo pip3 install -r requirements.txt \# 复制一份配置文件cp -rfp inventory/sample inventory/mycluster`
- ## 修改配置[​](\#修改配置 "修改配置的直接链接")  
  	    
  	  ^^需要修改的配置文件列表:^^  
  	    
  		* `inventory/mycluster/group_vars/all/*.yml`
  		* `inventory/mycluster/group_vars/k8s-cluster/*.yml`  
  
  下面介绍一些需要重点关注的配置，根据自己需求进行修改。
- ### 集群网络[​](\#集群网络 "集群网络的直接链接")  
  
  修改配置文件 `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml`:  
  
  `\# 选择网络插件，支持 cilium, calico, weave 和 flannel kube_network_plugin: cilium \# 设置 Service 网段 kube_service_addresses: 10.233.0.0/18 \# 设置 Pod 网段 kube_pods_subnet: 10.233.64.0/18 `  
  
  其它相关配置文件: `inventory/mycluster/group_vars/k8s_cluster/k8s-net-*.yml`。
- ### 运行时[​](\#运行时 "运行时的直接链接")  
  
  修改配置文件 `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml`:  
  
  `\# 支持 docker, crio 和 containerd，推荐 containerd. container_manager: containerd \# 是否开启 kata containers kata_containers_enabled: false `  
  
  其它相关配置文件:  
  
  `inventory/mycluster/group_vars/all/containerd.yml inventory/mycluster/group_vars/all/cri-o.yml inventory/mycluster/group_vars/all/docker.yml`
- ### 集群证书[​](\#集群证书 "集群证书的直接链接")  
  
  修改配置文件 `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml`:  
  
  `\# 是否开启自动更新证书，推荐开启。 auto_renew_certificates: true `
- ## 准备机器列表[​](\#准备机器列表 "准备机器列表的直接链接")  
  
  拿到集群部署的初始机器内网 ip 列表，修改 `inventory/mycluster/inventory.ini`:  
  
  `[all] master1 ansible_host=10.10.10.1master2 ansible_host=10.10.10.2master3 ansible_host=10.10.10.3node1 ansible_host=10.10.10.4node2 ansible_host=10.10.10.5node3 ansible_host=10.10.10.6node4 ansible_host=10.10.10.7node5 ansible_host=10.10.10.8node6 ansible_host=10.10.10.9node7 ansible_host=10.10.10.10[kube_control_plane] master1 master2 master3 [etcd] master1 master2 master3 [kube_node] master1 master2 master3 node1 node2 node3 node4 node5 node6 node7 [calico_rr] [k8s_cluster:children] kube_control_plane kube_node calico_rr`
  		  > **注:** 务必使用 `ansible_host` 标识节点内网 IP，否则可能导致出现类似 [这个issue](https://github.com/kubernetes-sigs/kubespray/issues/5949) 的问题。  
  附上 vim 编辑 inventory，批量加机器的技巧:  
  
  ![](https://proxy-prod.omnivore-image-cache.app/0x0,sWIJtokY1QvwXYZ-u9DF76t1W9CrTxABYwM82Bk7X5_I/https://image-host-1251893006.cos.ap-chengdu.myqcloud.com/2023/09/25/vim-inventory.gif)
- ## 国内环境安装[​](\#国内环境安装 "国内环境安装的直接链接")  
  
  在国内进行安装时，会因 GFW 影响而安装失败，参考 [kubespray 离线安装配置](https://imroc.cc/kubernetes/deploy/kubespray/offline)。
- ## 部署集群[​](\#部署集群 "部署集群的直接链接")  
  
  `ansible-playbook \ -i inventory/mycluster/inventory.ini \ --private-key=id_rsa \ --user=ubuntu -b \ cluster.yml`
- ## 获取 kubeconfig[​](\#获取-kubeconfig "获取 kubeconfig的直接链接")  
  
  部署完成后，从 master 节点上的 `/root/.kube/config` 路径获取到 kubeconfig，这里以 ansible 的 fetch 功能为例，将 kubeconfig 拷贝下来:  
  
  `$ ansible -i '10.10.6.9,' -b -m fetch --private-key id_rsa --user=ubuntu -a 'src=/root/.kube/config dest=kubeconfig flat=yes' all[WARNING]: Skipping callback plugin 'ara_default', unable to load 10.10.6.9 | CHANGED => { "changed": true, "checksum": "190eafeead70a8677b736eaa66d84d77c4a7f8be", "dest": "/root/kubespray/kubeconfig", "md5sum": "ded532f68930c48a53b3b2144b30f7f5", "remote_checksum": "190eafeead70a8677b736eaa66d84d77c4a7f8be", "remote_md5sum": null}`
  		  > `-i` 中的逗号是故意的，意思是不让 ansible 误以为是个 inventory 文件，而是解析为单个 host。  
  获取到 kubeconfig 后，可以修改其中的 server 地址，将 `https://127.0.0.1:6443` 改为非 master 节点可以访问的地址，最简单就直接替换 `127.0.0.1` 成其中一台 master 节点的 IP 地址，也可以在 Master 前面挂个负载均衡器，然后替换成负载均衡器的地址。
- ## 扩容节点[​](\#扩容节点 "扩容节点的直接链接")  
  
  如果要扩容节点，可以准备好节点的内网 IP 列表，并追加到之前的 inventory 文件里，然后再次使用 `ansible-playbook` 运行一次，有点不同的是: `cluster.yml` 换成 `scale.yml`:  
  
  `ansible-playbook \ -i inventory/mycluster/inventory.ini \ --private-key=id_rsa \ --user=ubuntu -b \ scale.yml`
- ## 缩容节点[​](\#缩容节点 "缩容节点的直接链接")  
  
  如果有节点不再需要了，我们可以将其移除集群，通常步骤是:  
  		1. `kubectl cordon NODE` 驱逐节点，确保节点上的服务飘到其它节点上去，参考 [安全维护或下线节点](https://imroc.cc/kubernetes/best-practices/ops/securely-maintain-or-offline-node)。
  		2. 停止节点上的一些 k8s 组件 (kubelet, kube-proxy) 等。
  		3. `kubectl delete NODE` 将节点移出集群。
  		4. 如果节点是虚拟机，并且不需要了，可以直接销毁掉。  
  
  前 3 个步骤，也可以用 kubespray 提供的 `remove-node.yml` 这个 playbook 来一步到位实现:  
  
  `ansible-playbook \ -i inventory/mycluster/inventory.ini \ --private-key=id_rsa \ --user=ubuntu -b \ --extra-vars "node=node1,node2" \ remove-node.yml`
  		  > `--extra-vars` 里写要移出的节点名列表，如果节点已经卡死，无法通过 SSH 登录，可以在 `--extra-vars` 加个 `reset_nodes=false` 的选项，跳过第二个步骤。  
  		* [原理](\#原理)
  		* [准备工作](\#准备工作)
  		* [修改配置](\#修改配置)
  			* [集群网络](\#集群网络)
  			* [运行时](\#运行时)
  			* [集群证书](\#集群证书)
  		* [准备机器列表](\#准备机器列表)
  		* [国内环境安装](\#国内环境安装)
  		* [部署集群](\#部署集群)
  		* [获取 kubeconfig](\#获取-kubeconfig)
  		* [扩容节点](\#扩容节点)
  		* [缩容节点](\#缩容节点)
	- ### 高亮
	  collapsed:: true
		- > 需要修改的配置文件列表: [⤴️](https://omnivore.app/me/kubespray-kubernetes-18e749f6773#e9858f36-d16e-4ddb-8b53-749cf7ca14ed)
		- > 下载 kubespray 并拷贝一份配置: [⤴️](https://omnivore.app/me/kubespray-kubernetes-18e749f6773#f97c37d3-89e3-48f9-8622-df4d14bc13eb)
	- [使用 HAProxy 设置高可用 Kubernetes 集群🇬🇧_kubernetes_weixin_0010034-K8S/Kubernetes](https://omnivore.app/me/ha-proxy-kubernetes-kubernetes-weixin-0010034-k-8-s-kubernetes-18e74a083e7)
	  collapsed:: true
	  site:: [devpress.csdn.net](https://devpress.csdn.net/k8s/62ee7c557e66823466182567.html)
	  author:: weixin_0010034 106人浏览 · 2022-08-06 22:36:05
	  date-saved:: [[Mar 25th, 2024]]
	  date-published:: [[Aug 6th, 2022]]
	- ### 内容
	  collapsed:: true
		- 这篇博文的主要目的是使用外部HAProxy设置 Kubernetes 集群的简单演练,这将是我们的kubectl客户端通信的端点。此设置的节点规格如下表所示。请记住,他们所有人都可以通过密码和无密码相互访问。 Kubernetes 集群将保持的环境运行在 OpenStack 上。这意味着一旦完成配置(ssh 密钥、主机等),例如 master 1,那么所有其他节点都可以通过 master 1 的快
		    
		  这篇博文的主要目的是使用外部HAProxy设置 Kubernetes 集群的简单演练,这将是我们的`kubectl`客户端通信的端点。此设置的节点规格如下表所示。请记住,他们所有人都可以通过密码和无密码相互访问。 Kubernetes 集群将保持的环境运行在 OpenStack 上。这意味着一旦完成配置(ssh 密钥、主机等),例如 master 1,那么所有其他节点都可以通过 master 1 的快照进行初始化。为了能够轻松设置这样的 Kubernetes 集群,我将使用KubeSpray这是一个存储库,其中包含设置必要集群所需的所有配置和剧本。  
		    
		  * 节点规格
		  * 先决条件
		  * 总览
		  * KubeSpray 配置
		  * 外部负载平衡器设置 (HAProxy)
		  * 设置 KubeSpray 配置  
		  
		  本演练的目的是在您自己的服务器中设置您自己的 Kubernetes 集群,这篇文章对于已经在使用云提供商解决方案的人来说不是很有用(Kubernetes 集群即服务)。您可以查看下面列出的以下资源:(其中很少 :))  
		  
		  云提供商解决方案:  
		  * Azure Kubernetes 服务 - AKS
		  * 谷歌 Kubernetes 引擎 - GKE
		  * DigitalOcean 上的托管 Kubernetes
		  * AWS 上的 Kubernetes  
		  
		  ^^Kubernetes 集群将在下表中的以下节点上设置,注意 HAProxy 将在另一个节点上运行,所有 ansible playbook 和设置 Kubernetes 集群将通过 HAProxy 管理。请记住,所有节点 + HAProxy 在内部都在同一子网下,这意味着我们将只有一个外部 IP 地址供 HAProxy 使用和^^`==kubectl==`^^个客户端通信。所有实例都在 ubuntu_18.04 上运行,这意味着说明和步骤可能不适用于其他系统。^^  
		  
		  $$![Alt](https://proxy-prod.omnivore-image-cache.app/0x0,sYdw5S2X_Oa2YlJg9f37EOWm0ZwS1fPv6VQBuFLKavX0/https://res.cloudinary.com/practicaldev/image/fetch/s--GUfyZg36--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/vb74by6zk4ewl0m1n4t1.png)$$ (https://res.cloudinary.com/practicaldev/image/fetch/s--GUfyZg36--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev- to-uploads.s3.amazonaws.com/i/vb74by6zk4ewl0m1n4t1.png)  
		  * 个节点  
		  
		  KubeSpray 的* 要求  
		  * 跨节点设置 SSH 密钥
		  * 获取快照(-可选-)
		  * 设置密码登录  
		  
		  下图是本演练结束时 Kubernetes 集群外观的总体概述,该图是集群的超级概览版本。  
		  
		  $$![概述集群](https://proxy-prod.omnivore-image-cache.app/0x0,sVnrqRJEwlib-j3LrfVKacYnYpP7f_PmW1KWIoyIvjps/https://res.cloudinary.com/practicaldev/image/fetch/s--0Rfj2kAF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://mrturkmencom.github.io/assets/images/kubernetes/overview-kube-cluster.png)$$ (https://res.cloudinary.com/practicaldev/image/fetch/s--0Rfj2kAF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://mrturkmencom.github .io/assets/images/kubernetes/overview-kube-cluster.png)  
		  
		  在上图中,节点没有任何外部 IP 地址,但是包括 HAProxy,它们都在同一个子网中,只有 HAProxy 有外部 IP 地址,可以被`kubectl`个客户端访问。  
		  
		  在移动 Kubernetes 集群的安装步骤之前,我们需要设置一个具有预定义配置的示例主节点(实例)。由于我们只有一台对外开放的服务器,因此我们需要确保 HAProxy 和示例主节点之间存在连接。我目前将其称为示例主节点,这是因为将配置所有初步配置,例如使用密码进行身份验证、禁用交换区域和 ssh 密钥。这个示例主节点应该通过 HAProxy 启动和访问,这意味着为了访问示例主节点,我应该执行以下操作;  
		  * SSH 到 HAProxy 使用 SSH 密钥(**密码登录已禁用**),如`ssh -i ~/.ssh/id_rsa <username>@<ha-proxy-external-ip>`
		  * 将 SSH 密钥复制到 HAProxy,让您进入示例主节点
		  * 然后 SSH 以相同的方法采样主节点。 (`ssh ~/.ssh/masternode.pem <username>@<master-node-ip>`  
		  
		  在您进入示例主节点之后,现在应该进行一些配置和设置。之后,我们可以从配置的示例主节点的快照中初始化其他五个节点。  
		  
		  脚步:  
		  * **如果尚未启用,请启用密码登录。**  
		  
		  $ echo "PermitRootLogin yes" >> /etc/ssh/sshd_config  
		  $ sed -i -E 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config  
		  
		  进入全屏模式 退出全屏模式  
		  * **指定ROOT密码**  
		  
		  $ sudo su  
		  $ passwd  
		  
		  进入全屏模式 退出全屏模式  
		  
		  给定的命令将询问 root 用户的新 unix 密码。定义密码,不要忘记或丢失它。由于我们将使用这台已配置机器的快照,因此所有设置都将相同,我确实喜欢这样来简化流程。  
		  * **禁用交换区(以根目录运行所有命令)**  
		  
		  $ swapoff -a  
		  
		  进入全屏模式 退出全屏模式  
		  
		  之后,退出示例主节点,创建该节点的快照(在 OpenStack 中称为卷快照),一旦成功创建快照,其他五个节点都应从该示例主节点的快照初始化。这样,无需重复上述相同的步骤。  
		  
		  如果无法创建快照,请按照给定的步骤操作(如果只有,您不能创建快照并从快照初始化其他五个节点)  
		  
		  创建所有节点(worker 和 master)  
		  
		  启用从 HAPRoxy 服务器到所有节点的 SSH 连接。  
		  
		  从 HAProxy 服务器,执行以下步骤。 (-确保您已经配置了具有 ROOT 权限的 SSH 连接,并且可以从 HAProxy 节点访问所有节点 -)  
		  
		  一旦您确定您可以通过 SSH 从 HAProxy 访问所有节点,请执行以下步骤。  
		  * **安装parallel-ssh(-在节点上并行运行命令-)(以ROOT权限运行)**  
		  
		  $ apt-get update && apt-get install -y pssh  
		  
		  进入全屏模式 退出全屏模式  
		  * **安装 HAProxy(作为 ROOT 权限)**  
		  
		  $ apt-get install -y haproxy  
		  
		  进入全屏模式 退出全屏模式  
		  * **修改`/etc/hosts`(-为了方便通过节点通信-)**  
		  
		  *将工作节点和主节点 IP 附加到`/etc/hosts`文件*  
		  
		  $ vim /etc/hosts
		  
		  10.0.128.156 worker3  
		  10.0.128.137 worker2  
		  10.0.128.81 worker1  
		  10.0.128.184 master3  
		  10.0.128.171 master2  
		  10.0.128.149 master1  
		    
		  进入全屏模式 退出全屏模式  
		    
		  * **在主目录上创建`nodes`文本文件**  
		  
		  $ cat nodes  
		  worker3  
		  worker2  
		  worker1  
		  master3  
		  master2  
		  master1  
		  
		  进入全屏模式 退出全屏模式  
		  
		  _由于它们的IP地址在`/etc/hosts`文件中定义,系统现在可以通过名称识别和连接它们的IP_  
		  * **生成 SSH 密钥并将其复制到所有节点**(方便通信所需)  
		  
		  如果已经有 SSH 密钥(如`~/.ssh/id_rsa`),您也可以使用它。如果没有,您可以执行以下步骤  
		  
		  $ ssh-keygen  
		  
		  $ for i in $(cat nodes); ssh-copy-id $i; done  
		  
		  进入全屏模式 退出全屏模式  
		  
		  作为第二个命令给出的 for 循环会将 ssh 密钥复制到所有节点,然后在没有密码的情况下访问任何节点将是完美的。就像下面给出的命令;  
		  
		  $ ssh master1  
		  
		  进入全屏模式 退出全屏模式  
		  * **禁用所有节点上的交换区(注意,如果使用快照方法,则无需执行此步骤)**  
		  
		  $ parallel-ssh -h nodes -i "swapoff -a"  
		  
		  进入全屏模式 退出全屏模式  
		  
		  并行 SSH 工具可以方便地为多个主机并行完成任务。  
		  
		  KubeSpray 是一个使用 Ansible playbook 使用预定义配置设置设置 Kubernetes 集群的存储库。 KubeSpray 的使用非常简单,作为默认设置,KubeSpray 在每个工作节点中使用内部负载均衡器,这意味着当您使用 KubeSpray 的默认值设置 Kubernetes 集群时,您将获得以下架构概述。  
		  
		  $$![使用 KubeSpray 设置的默认 Kubernetes Arch](https://proxy-prod.omnivore-image-cache.app/0x0,sZsN1GPnpb5slBn98wl_r39y9dFf3TtakFH_wDZ6YpLU/https://res.cloudinary.com/practicaldev/image/fetch/s--3VWtC4aK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://mrturkmencom.github.io/assets/images/kubernetes/overview-kube-cluster-default.png)$$ (https://res.cloudinary.com/practicaldev/image/fetch/s--3VWtC4aK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https:// mrturkmencom.github.io/assets/images/kubernetes/overview-kube-cluster-default.png)  
		  
		  但是,在本指南中,外部负载均衡器方法将用于设置集群,如果您希望使用 KubeSpray 将所有内容保留为默认设置,您可以跳过此External Load Balancer Setup部分。  
		  
		  修改 HAProxy 的配置文件以启用外部 LoadBalancer,复制以下配置并附加到`/etc/haproxy/haproxy.cfg`。(文件结尾)  
		  
		  listen kubernetes-apiserver-https  
		  bind <your-haproxy-internal-ip>:8383  
		  mode tcp  
		  option log-health-checks  
		  timeout client 3h  
		  timeout server 3h  
		  server master1 <your-master1-ip>:6443 check check-ssl verify none inter 10000  
		  server master2 <your-master2-ip>:6443 check check-ssl verify none inter 10000  
		  server master3 <your-master3-ip>:6443 check check-ssl verify none inter 10000  
		  balance roundrobin  
		  
		  进入全屏模式 退出全屏模式  
		  
		  平衡算法是`roundrobin`,但是您可以从 HAProxy 提供的可用平衡算法列表中更改它。  
		  
		  完成后,保存并重新启动 HAProxy 服务。  
		  
		  $ systemctl restart haproxy  
		  
		  进入全屏模式 退出全屏模式  
		  
		  由于将使用外部负载均衡器,因此无需做任何事情来更改 KubeSpray 中的默认值。以下步骤将在 HAProxy 节点上完成。  
		  * **克隆项目并准备环境**  
		  
		  
		  $ git clone https://github.com/kubernetes-sigs/kubespray  
		  $ apt-get install -y python3-pip  
		  $ cd kubespray  
		  
		  进入全屏模式 退出全屏模式  
		  * **按照 KubeSpray README.md 文件中的指南进行操作**  
		  
		  遵循从KubeSpray README.md获取的说明  
		  
		  
		  sudo pip3 install -r requirements.txt  
		  
		  cp -rfp inventory/sample inventory/mycluster  
		  
		  declare -a IPS=(10.0.128.149 10.0.128.171 10.0.128.184 10.0.128.81 10.0.128.137 10.0.128.156)  
		  CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}  
		  
		  进入全屏模式 退出全屏模式  
		  * **修改生成主机 YAML 文件**  
		  
		  当您检查`inventory/mycluster/hosts.yaml`文件时,您会注意到它创建了两个主节点,我们需要三个,将缺少的一个正确添加到该列表中,如下所示。  
		  
		  all:  
		  hosts:  
		  master1:  
		  ansible_host: 10.0.128.149  
		  ip: 10.0.128.149  
		  access_ip: 10.0.128.149  
		  master2:  
		  ansible_host: 10.0.128.171  
		  ip: 10.0.128.171  
		  access_ip: 10.0.128.171  
		  master3:  
		  ansible_host: 10.0.128.184  
		  ip: 10.0.128.184  
		  access_ip: 10.0.128.184  
		  worker1:  
		  ansible_host: 10.0.128.81  
		  ip: 10.0.128.81  
		  access_ip: 10.0.128.81  
		  worker2:  
		  ansible_host: 10.0.128.137  
		  ip: 10.0.128.137  
		  access_ip: 10.0.128.137  
		  worker3:  
		  ansible_host: 10.0.128.156  
		  ip: 10.0.128.156  
		  access_ip: 10.0.128.156  
		  children:  
		  kube-master:  
		  hosts:  
		  master1:  
		  master2:  
		  master3:  
		  kube-node:  
		  hosts:  
		  master1:  
		  master2:  
		  master3:  
		  worker1:  
		  worker2:  
		  worker3:  
		  etcd:  
		  hosts:  
		  master1:  
		  master2:  
		  master3:  
		  k8s-cluster:  
		  children:  
		  kube-master:  
		  kube-node:  
		  calico-rr:  
		  hosts: {}  
		  
		  进入全屏模式 退出全屏模式  
		  
		  完成后,应修改以使用外部负载均衡器 HAProxy 的另一件事是位于`inventory/mycluster/group_vars/all/`下的`all.yaml`文件。  
		  
		  `all.yml`是指定集群主要配置的通用配置文件,它默认使用 Nginx 负载均衡器,这意味着每个工作节点都有自己的本地 nginx 负载均衡器,如上图第二个所示。如果没有指定其他任何内容。  
		  * **禁用默认负载均衡器**  
		  
		  $ vim inventory/mycluster/group_vars/all/all.yml  
		  loadbalancer_apiserver_localhost: false  
		  
		  进入全屏模式 退出全屏模式  
		  * **添加外部负载均衡器 HAProxy。**  
		  
		  $ vim inventory/mycluster/group_vars/all/all.yml
- ## External LB example config  
  apiserver_loadbalancer_domain_name: "<domain-name-of-lb>"  
  loadbalancer_apiserver:  
  address: 10.0.128.193  
  port: 8383  
  
  进入全屏模式 退出全屏模式  
  		* **初始化集群部署**  
  
  
  $ ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml  
  
  进入全屏模式 退出全屏模式  
  
  这大约需要 10-15 分钟,具体取决于您的集群,如果一切顺利,在通过 Ansible 部署结束时,您将不会遇到任何问题。如果是这样,您可以通过 SSH 到`master`节点进行测试,然后尝试`kubectl cluster-info`。  
  
  $ kubectl cluster-info  
  Kubernetes master is running at .....  
  
  To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  
  
  进入全屏模式 退出全屏模式  
  
  这意味着 Kubernetes 集群具有三个主节点和三个工作节点可供使用。  
  
  请注意,集群的默认配置可以进行更多更改,但是在尝试更改默认配置之前,请确保您对 KubeSpray 默认设置的更改内容进行了正确的研究。否则,可能会出现有关自定义配置设置的问题。  
  
  有关更多信息,请保持更新并观看KubeSpray关于问题、陷阱等的信息。  
  
  这篇文章的最后一步是为您的个人/工作计算机创建`kubectl`配置以访问集群。在您的环境中安装 kubectl。然后将配置从主节点复制到您的`~/.kube/`作为`config`。  
  
  由于我们只有一个端点,因此应将配置文件复制到 HAProxy 服务器,然后通过`rsync`或`scp`复制到您的计算机  
  		* **在 HAProxy 服务器上**  
  
  $ scp root@master1:/etc/kubernetes/admin.conf config  
  $ cp config /home/ubuntu/  
  $ chown ubuntu:ubuntu /home/ubuntu/config  
  
  进入全屏模式 退出全屏模式  
  		* **在您的个人/工作计算机上**  
  
  $ scp -i ~/.ssh/haproxy.pem ubuntu@<ha-proxy-ip>:/home/ubuntu/config ~/.kube/  
  
  进入全屏模式 退出全屏模式  
  
  现在,您应该能够像在主节点中一样获取和转储集群信息。  
  
  $ kubectl cluster-info  
  Kubernetes master is running at .....  
  
  To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  
  
  进入全屏模式 退出全屏模式  
  
  关于 Kubernetes 集群环境有很多配置和不同的设置,通常使用 Cloud Provider 解决方案不会那么痛苦或轻松。但是,有时设置自己的环境成本更低,并且可以完全访问任何内容,这对于在后台学习或创建高度定制的环境可能会更好。这实际上取决于您的情况,因此您可以自行设置 Kubernetes 集群或将其用作云提供商的服务。  
  
  顺便说一句,感谢您抽出时间查看帖子😉  
  
  [![Logo](https://proxy-prod.omnivore-image-cache.app/0x0,sMSEWI1yFsHJDbFTg6k6s_vj3dsjjaRp38r9xRpOf9W4/https://devpress.csdnimg.cn/a6de972e90a648af920342860ed94803.jpg)](https://devpress.csdn.net/k8s)  
  
  K8S/Kubernetes社区为您提供最前沿的新闻资讯和知识内容
	- ### 高亮
	  collapsed:: true
		- > Kubernetes 集群将在下表中的以下节点上设置,注意 HAProxy 将在另一个节点上运行,所有 ansible playbook 和设置 Kubernetes 集群将通过 HAProxy 管理。请记住,所有节点 + HAProxy 在内部都在同一子网下,这意味着我们将只有一个外部 IP 地址供 HAProxy 使用和`kubectl`个客户端通信。所有实例都在 ubuntu_18.04 上运行,这意味着说明和步骤可能不适用于其他系统。 [⤴️](https://omnivore.app/me/ha-proxy-kubernetes-kubernetes-weixin-0010034-k-8-s-kubernetes-18e74a083e7#6d2ea670-2186-4e4d-a559-e0d071078594)
	- [多种方法部署Pandora，让ChatGPT更好用 - 梅塔沃克 - 专注跨境](https://omnivore.app/me/https-iweec-com-882-html-188852229b8)
	  site:: [iweec.com](https://iweec.com/882.html)
	  date-saved:: [[Jun 4th, 2023]]
	  date-published:: [[May 28th, 2023]]
	  id:: 447687b8-77f8-4390-adaa-e1e6735771fb
	  collapsed:: true
	- ### 内容
	  collapsed:: true
		- [ ![](https://proxy-prod.omnivore-image-cache.app/0x0,sRB4O0POGHbGe4TXAPfOvb1Pu5zJPhT8ehNq9cQ6Z8b8/https://iweec.com/laika.jpg)](https://www.lcayun.com/aff/SWTQTESR)
		    
		  Pandora不愧是这三个月以来最为突出的项目之一，它完美解决了ChatGPT使用中经常遇到的种种问题，而且部署是相当简单，方便。项目地址：[https://github.com/pengzhile/pandora](https://iweec.com/go/aHR0cHM6Ly9naXRodWIuY29tL3Blbmd6aGlsZS9wYW5kb3Jh)  
		    
		  另外，今天（5.28）GPT再一次大范围风控，这似乎与sentry.io有一定的关联性，Pandora也可以避免封号的问题，当然你如果用全局模式问题不大。
- ## Pandora能做什么  
  	    
  	  一、本地部署C^^hatGPT，Pandora绕^^过 Cloudflare，可以把ChatGPT部署在自己的电脑上，使用127.0.0.1即可使用官方的GPT（仅需登陆一次，即可正常使用14天左右）；  
  	    
  	  二、^^服务器部署ChatGPT，我们可以使用同样的方法，将Pandora程序安装在vps上，甚至可以使用自己的域名，无需科学上网即可使用，不必担心IP问题^^；  
  	    
  	  三、我们可以使用Pandora，用多种方式使用GPT，例如命令行模式，web模式，其中web页面使用方法与官网一致，还能保存记录，非常完美。
- ## Pandora部署方法
- ## 一、部署在本地电脑上  
  	    
  	  无论是windows、macOS或者linux系统，都可以用三个步骤把Pandora部署到本地：  
  	    
  	  1、安装docker并启动，到[https://www.docker.com/get-started](https://iweec.com/go/aHR0cHM6Ly93d3cuZG9ja2VyLmNvbS9nZXQtc3RhcnRlZA==) 下载docker；安装后启动；  
  	    
  	  2、命令行执行：  
  	    
  	  ```angelscript
  	  docker pull pengzhile/pandora
  	  docker run  -e PANDORA_CLOUD=cloud -e PANDORA_SERVER=0.0.0.0:8899 -p 8899:8899 -d pengzhile/pandora
  	  ```
  	    
  	  3、^^本地浏览器访问 127.0.0.1:8899  即可访问，直接登陆或者使用Access Token，然后就能够正常的使用GPT了。^^  
  	    
  	  ![截屏2023-05-28 11.22.28.png](https://proxy-prod.omnivore-image-cache.app/0x0,se6S4jtAxJPrywnpRsw5CoUZDtOmmRZhr7n6t-2Fw8Rs/https://iweec.com/usr/uploads/2023/05/1477783447.png "截屏2023-05-28 11.22.28.png")  
  	    
  	  4、作者给出的获得Access Token的地址：[http://chat.openai.com/api/auth/session](https://iweec.com/go/aHR0cDovL2NoYXQub3BlbmFpLmNvbS9hcGkvYXV0aC9zZXNzaW9u)   
  	    
  	  缺点：登陆或者获取Token一样需要外网；
- ## 二、部署在vps上  
  	    
  	  ^^强调：在vps部署Pandora请使用国外主机，这样无论是电脑还是手机都可以使用GPT了，推荐：^^  
  	    
  	  1、安装docker环境（ubuntu系统）  
  	    
  	  ```sas
  	  apt update && apt install docker.io -y
  	  ```
  	    
  	  2、拉取镜像  
  	    
  	  ```nginx
  	  docker pull pengzhile/pandora
  	  ```
  	    
  	  3、启动容器  
  	    
  	  （1）命令行模式：  
  	    
  	  ```applescript
  	  docker run -it --rm pengzhile/pandora
  	  ```
  	    
  	  （2）web模式  
  	    
  	  ```angelscript
  	  
  	  docker run  -e PANDORA_CLOUD=cloud -e PANDORA_SERVER=0.0.0.0:8899 -p 8899:8899 -d pengzhile/pandora
  	  ```
  	    
  	  用这种方法搭建，流畅使用GPT，任何终端的浏览器均可使用，安全、方便。
- ## 三、绑定域名的方法  
  	    
  	  1、如果你使用的是nginx，编辑配置文件  
  	    
  	  ```vim
  	  vim /etc/nginx/nginx.conf
  	  ```
  	    
  	  将其内容替换为：  
  	    
  	  ```nginx
  	  events {}
  	  
  	  http {
  	      server {
  	          listen 80;
  	          server_name fboth.pp.ua;
  	  
  	          location / {
  	              proxy_pass http://127.0.0.1:8899;
  	          }
  	      }
  	  }
  	  
  	  ```
  	    
  	  如果你想用caddy，编辑 /etc/caddy/Caddyfile  
  	    
  	  ```css
  	  xx.com  \#这里是你解析的域名
  	  encode gzip
  	  reverse_proxy 127.0.0.1:8899
  	  ```
- ## 总结  
  	    
  	  Pandora不愧是一个优秀的项目，除了docker安装，还可以pip，解决了GPT使用上的诸多问题，而且速度比官网还快，值得点赞！
	- ### 高亮
		- > 强调：在vps部署Pandora请使用国外主机，这样无论是电脑还是手机都可以使用GPT了，推荐： [⤴️](https://omnivore.app/me/https-iweec-com-882-html-188852229b8#c9d96b93-1de0-4fb7-a95e-cb02b444f96d)
		- > hatGPT，Pandora绕 [⤴️](https://omnivore.app/me/https-iweec-com-882-html-188852229b8#e0d91ccb-df2a-496e-b165-4f6246d2f389)
		- > 服务器部署ChatGPT，我们可以使用同样的方法，将Pandora程序安装在vps上，甚至可以使用自己的域名，无需科学上网即可使用，不必担心IP问题 [⤴️](https://omnivore.app/me/https-iweec-com-882-html-188852229b8#a946dc02-48d7-4c67-8a11-a50c8d36e9ab)
		- > 本地浏览器访问 127.0.0.1:8899 即可访问，直接登陆或者使用Access Token，然后就能够正常的使用GPT了。 [⤴️](https://omnivore.app/me/https-iweec-com-882-html-188852229b8#5f2239af-7233-484c-8c07-f954f7630d32)